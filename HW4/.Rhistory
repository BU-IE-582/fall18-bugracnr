cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
cv_model$pred
penalized_model <- glmnet(x = as.matrix(train.digit[,-65]), y = train.digit$class, family = "multinomial")
results <- predict(penalized_model,newx =  as.matrix(test.digit[,-65]), s = 0.01, type = "class")
table(results, test.digit$class)
sum(results==test.digit$class)/nrow(test.digit)
###Parkinsons
parkinsons <- as.data.table(read.csv("Data/Parkinsons/parkinsons_updrs.csv", header = TRUE))
parkinsons[,subject. := NULL]
parkinsons[,motor_UPDRS := NULL]
test.index <- sample(nrow(parkinsons), 500)
train.parkinsons <- parkinsons[-test.index,]
test.parkinsons <- parkinsons[test.index,]
####### treee
tree.parkinsons <- tree(total_UPDRS~., data=train.parkinsons)
parkinsons.pred <- predict(tree.parkinsons, test.parkinsons)
sum((parkinsons.pred-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
######### svm
svmfit <- svm(total_UPDRS~., data = train.parkinsons, kernel = "linear", cost = 0.1, scale = FALSE)
svmpred <- predict(svmfit, test.parkinsons[,-c("total_UPDRS")])
sum((svmpred-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
###### rf
rf_anneal <- randomForest(train.parkinsons[,-c("total_UPDRS")], y = train.parkinsons$total_UPDRS, ntree = 500, proximity = TRUE)
prob_classes <- predict(rf_anneal, test.parkinsons[,-c("total_UPDRS")])
sum((prob_classes-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
prob_classes <- predict(rf_anneal, test.parkinsons[,-c("total_UPDRS")])
sum((prob_classes-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
testlabel <- as.factor(test.parkinsons$total_UPDRS)
trainlabel <- as.factor(train.parkinsons$total_UPDRS)
train_matrix <- xgb.DMatrix(data = as.matrix(train.parkinsons[,-65]), label = trainlabel)
test_matrix <- xgb.DMatrix(data = as.matrix(test.parkinsons[,-65]), label = testlabel)
numberOfClasses <- length(unique(testlabel))
xgb_params <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
data = train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
cv_model$pred
OOF_prediction <- data.frame(cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = trainlabel + 1)
head(OOF_prediction)
confusionMatrix(factor(OOF_prediction$max_prob),
factor(OOF_prediction$label),
mode = "everything")
boost.model <- xgboost(data = as.matrix(train.parkinsons[,-c("total_UPDRS")]), label = train.parkinsons$total_UPDRS, nrounds = 50)
boost.pred <- predict(boost.model, as.matrix(test.parkinsons[,-c("total_UPDRS")]))
sum((boost.pred-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
penalized_model <- glmnet(x = as.matrix(train.parkinsons[,-c("total_UPDRS")]), y = train.parkinsons$total_UPDRS, family = "gaussian")
results <- predict(penalized_model,newx =  as.matrix(test.parkinsons[,-c("total_UPDRS")]), s = 0.01, type = "class")
sum((results-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
require(farff)
require(data.table)
require(nnet)
require(glmnet)
require(e1071)
require(caret)
library(tree)
require(gbm)
require(xgboost)
require(e1071)
require(Ckmeans.1d.dp)
library(randomForest)
require(fastDummies)
require(dplyr)
require(glmnet)
######### DATA PREPROCESSING
#### ANNEALING DATA
anneal.train <- as.data.table(read.csv("Data/Anneal/anneal_train.csv", header = FALSE))
anneal.test <- as.data.table(read.csv("Data/Anneal/anneal_test.csv", header = FALSE))
names.anneal <- (read.csv("Data/Anneal/anneal_names.csv", header = FALSE))
names.anneal <- as.character(names.anneal$V1)
names.anneal <- c(names.anneal, "class")
colnames(anneal.train) <- names.anneal
colnames(anneal.test) <- names.anneal
set.seed(5)
anneal.index <- sample(nrow(anneal.train), 150)
anneal.test <- rbind(anneal.train[anneal.index,], anneal.test)
anneal.train <- anneal.train[-anneal.index,]
anneal.train_class <- as.factor(anneal.train$class)
anneal.test_class <- as.factor(anneal.test$class)
names <- as.data.table(cbind(names.anneal, ifelse(n <- sapply(anneal.train, function(x) length(levels(x))) == 1, "DROP", "NODROP")))
#### factor problem
rm(names)
#### factor problem
anneal.names <- as.data.table(cbind(names.anneal, ifelse(n <- sapply(anneal.train, function(x) length(levels(x))) == 1, "DROP", "NODROP")))
anneal.names <- anneal.names[V2 == "NODROP"]
anneal.names <- anneal.names$names
anneal.x <- anneal.train[,..names]
anneal.x <- anneal.train[,..anneal.names]
anneal.x <- anneal.x[,1:(ncol(anneal.x)-1)]
anneal.y <- anneal.test[,..anneal.names]
anneal.y <- anneal.y[,1:(ncol(anneal.y)-1)]
####### treee
tree.anneal <- tree(anneal.train_class~., anneal.train[,-39])
tree.pred <- predict(tree.anneal, anneal.test, type = "class")
table(tree.pred,anneal.test$class)
## end of factor problem
rm(tree.pred)
####### treee
tree.anneal <- tree(anneal.train_class~., anneal.train[,-39])
anneal.tree.pred <- predict(tree.anneal, anneal.test, type = "class")
table(anneal.tree.pred,anneal.test$class)
sum(anneal.tree.pred==anneal.test$class)/nrow(anneal.test)
######### svm
anneal.svmfit <- svm(anneal.train_class~., data = anneal.x, kernel = "linear", cost = 0.1, scale = TRUE)
anneal.svmpred <- predict(anneal.svmfit, anneal.y)
table(anneal.svmpred,anneal.test$class)
sum(anneal.svmpred==anneal.test$class)/nrow(anneal.test)
anneal.train_class <- as.factor(anneal.train$class)
rf_anneal <- randomForest(anneal.train[,-39], y = anneal.train_class, ntree = 100, proximity = TRUE)
anneal.prob_classes <- predict(rf_anneal, anneal.test[,-39])
table(anneal.prob_classes, anneal.test$class)
sum(anneal.prob_classes==anneal.test$class)/nrow(anneal.test)
#### boost
anneal.a <- rbind(anneal.train[,-39],anneal.test[,-39])
anneal.a <- dummy_cols(anneal.a, remove_first_dummy = TRUE)
anneal.a <- anneal.a[,-c(1:38)]
anneal.x1 <- anneal.a[1:648,]
anneal.y1 <- anneal.a[649:898,]
anneal.x1 <- as.matrix(anneal.x1)
anneal.y1 <- as.matrix(anneal.y1)
anneal.trainlabel <- (anneal.train$class-1)
anneal.testlabel <- (anneal.test$class-1)
anneal.train_matrix <- xgb.DMatrix(data = anneal.x1, label = anneal.trainlabel)
anneal.test_matrix <- xgb.DMatrix(data = anneal.y1, label = anneal.testlabel)
numberOfClasses <- length(unique(anneal.testlabel))
xgb_params <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
anneal.cv_model <- xgb.cv(params = xgb_params,
data = anneal.train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
anneal.cv_model$pred
anneal.OOF_prediction <- data.frame(anneal.cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = anneal.trainlabel + 1)
head(anneal.OOF_prediction)
confusionMatrix(factor(anneal.OOF_prediction$max_prob),
factor(anneal.OOF_prediction$label),
mode = "everything")
anneal.penalized.x1 <- anneal.a[1:648,]
anneal.penalized.y1 <- anneal.a[649:898,]
anneal.penalized_model <- glmnet(x = as.matrix(anneal.penalized.x1), y = anneal.train_class, family = "multinomial")
results <- predict(anneal.penalized_model,newx =  as.matrix(anneal.penalized.y1), s = 0.01, type = "class")
### penalized regression
rm(results)
anneal.results <- predict(anneal.penalized_model,newx =  as.matrix(anneal.penalized.y1), s = 0.01, type = "class")
table(anneal.results, anneal.test_class)
sum(anneal.results==anneal.test_class)/nrow(anneal.penalized.y1)
###Digits
train.digit <- read.csv("Data/Digits/optdigits_train.csv", header = FALSE)
test.digit <- read.csv("Data/Digits/optdigits_test.csv", header = FALSE)
names <- c("col1","col2","col3","col4","col5","col6","col7","col8","col9","col10","col11","col12","col13","col14","col15","col16","col17","col18","col19","col20","col21","col22","col23","col24","col25","col26","col27","col28","col29","col30","col31","col32","col33","col34","col35","col36","col37","col38","col39","col40","col41","col42","col43","col44","col45","col46","col47","col48","col49","col50","col51","col52","col53","col54","col55","col56","col57","col58","col59","col60","col61","col62","col63","col64","class")
colnames(train.digit) <- names
colnames(test.digit) <- names
train.digit$class <- as.factor(train.digit$class)
test.digit$class <- as.factor(test.digit$class)
####### treee
tree.digit <- tree(class~., data=train.digit)
digit.pred <- predict(tree.digit, test.digit, type = "class")
table(digit.pred,test.digit$class)
sum(digit.pred==test.digit$class)/nrow(test.digit)
######### svm
digit.svmfit <- svm(class~., data = train.digit, kernel = "linear", cost = 0.1, scale = FALSE)
digit.svmpred <- predict(digit.svmfit, test.digit[,-65])
table(digit.svmpred,test.digit$class)
sum(digit.svmpred==test.digit$class)/nrow(test.digit)
###### rf
rf_digit <- randomForest(train.digit[,-65], y = train.digit$class, ntree = 100, proximity = TRUE)
prob_classes <- predict(rf_digit, test.digit[,-65])
rm(prob_classes)
###### rf
rf_digit <- randomForest(train.digit[,-65], y = train.digit$class, ntree = 100, proximity = TRUE)
digit.prob_classes <- predict(rf_digit, test.digit[,-65])
table(digit.prob_classes, test.digit$class)
sum(digit.prob_classes==test.digit$class)/nrow(test.digit)
digit.testlabel <- test.digit$class
digit.trainlabel <- train.digit$class
digit.train_matrix <- xgb.DMatrix(data = as.matrix(train.digit[,-65]), label = digit.trainlabel)
digit.test_matrix <- xgb.DMatrix(data = as.matrix(test.digit[,-65]), label = digit.testlabel)
numberOfClasses <- length(unique(digit.testlabel))
xgb_params <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = numberOfClasses)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
digit.cv_model <- xgb.cv(params = xgb_params,
data = digit.train_matrix,
nrounds = nround,
nfold = cv.nfold,
verbose = FALSE,
prediction = TRUE)
digit.cv_model$pred
digit.OOF_prediction <- data.frame(digit.cv_model$pred) %>%
mutate(max_prob = max.col(., ties.method = "last"),
label = digit.trainlabel + 1)
head(digit.OOF_prediction)
confusionMatrix(factor(digit.OOF_prediction$max_prob),
factor(digit.OOF_prediction$label),
mode = "everything")
boost.digit <- xgboost(as.matrix(train.digit[,-65]), digit.trainlabel, nrounds =50, type = "class")
boost.digit.pred <- predict(boost.digit, as.matrix(test.digit[,-65]), type = "class")
table(boost.digit.pred, digit.testlabel)
table(boost.digit.pred, digit.testlabel)
sum(boost.digit.pred==digit.testlabel)/nrow(test.digit)
digit.penalized_model <- glmnet(x = as.matrix(train.digit[,-65]), y = train.digit$class, family = "multinomial")
digit.penalized_model <- glmnet(x = as.matrix(train.digit[,-65]), y = train.digit$class, family = "multinomial")
digit.results <- predict(digit.penalized_model,newx =  as.matrix(test.digit[,-65]), s = 0.01, type = "class")
table(digit.results, test.digit$class)
sum(digit.results==test.digit$class)/nrow(test.digit)
###Parkinsons
parkinsons <- as.data.table(read.csv("Data/Parkinsons/parkinsons_updrs.csv", header = TRUE))
parkinsons[,subject. := NULL]
parkinsons[,motor_UPDRS := NULL]
set.seed(19)
parkinsons.test.index <- sample(nrow(parkinsons), 500)
train.parkinsons <- parkinsons[-parkinsons.test.index,]
test.parkinsons <- parkinsons[parkinsons.test.index,]
####### treee
tree.parkinsons <- tree(total_UPDRS~., data=train.parkinsons)
parkinsons.pred <- predict(tree.parkinsons, test.parkinsons)
sum((parkinsons.pred-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
######### svm
parkinsons.svmfit <- svm(total_UPDRS~., data = train.parkinsons, kernel = "linear", cost = 0.1, scale = FALSE)
parkinsons.svmpred <- predict(parkinsons.svmfit, test.parkinsons[,-c("total_UPDRS")])
sum((parkinsons.svmpred-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
###### rf
rf_parkinsons <- randomForest(train.parkinsons[,-c("total_UPDRS")], y = train.parkinsons$total_UPDRS, ntree = 100, proximity = TRUE)
parkinsons.prob_classes <- predict(rf_parkinsons, test.parkinsons[,-c("total_UPDRS")])
sum((parkinsons.prob_classes-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
parkinsons.penalized_model <- glmnet(x = as.matrix(train.parkinsons[,-c("total_UPDRS")]), y = train.parkinsons$total_UPDRS, family = "gaussian")
parkinsons.results <- predict(parkinsons.penalized_model,newx =  as.matrix(test.parkinsons[,-c("total_UPDRS")]), s = 0.01, type = "class")
sum((parkinsons.results-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
######### BANKRUPTCY
bankruptcy <- as.data.table(readARFF("Data/Bankruptcy/1year.arff"))
set.seed(15)
bankruptcy <- bankruptcy[complete.cases(bankruptcy)]
bankruptcy.test.index <- sample(nrow(bankruptcy), 750)
train.bankruptcy <- bankruptcy[-bankruptcy.test.index,]
test.bankruptcy <- bankruptcy[bankruptcy.test.index,]
####### treee
tree.bankruptcy <- tree(class~., data=train.bankruptcy)
bankruptcy.pred <- predict(tree.bankruptcy, test.bankruptcy[,-c("class")], type = "class")
table(bankruptcy.pred,test.bankruptcy$class)
sum(bankruptcy.pred==test.bankruptcy$class)/nrow(test.bankruptcy)
######### svm
bankruptcy.svmfit <- svm(class~., data = train.bankruptcy, kernel = "linear", cost = 0.1, scale = TRUE)
bankruptcy.svmpred <- predict(bankruptcy.svmfit, test.bankruptcy[,-c("class")])
table(bankruptcy.svmpred,test.bankruptcy$class)
sum(bankruptcy.svmpred==test.bankruptcy$class)/nrow(test.bankruptcy)
###### rf
rf_bankruptcy <- randomForest(train.bankruptcy[,-c("class")], y = train.bankruptcy$class, ntree = 500, proximity = TRUE)
bankruptcy.prob_classes <- predict(rf_bankruptcy, test.bankruptcy[,-c("class")])
table(bankruptcy.prob_classes, test.bankruptcy$class)
sum(bankruptcy.prob_classes==test.bankruptcy$class)/nrow(test.bankruptcy)
bankruptcy.penalized_model <- glmnet(x = as.matrix(train.bankruptcy[,-c("class")]), y = train.bankruptcy$class, family = "multinomial")
bankruptcy.results <- predict(penalized_model,newx =  as.matrix(test.bankruptcy[,-c("class")]), s = 0.01, type = "class")
table(bankruptcy.results, test.bankruptcy$class)
sum(bankruptcy.results==test.bankruptcy$class)/nrow(test.bankruptcy)
bankruptcy.results <- predict(bankruptcy.penalized_model,newx =  as.matrix(test.bankruptcy[,-c("class")]), s = 0.01, type = "class")
table(bankruptcy.results, test.bankruptcy$class)
sum(bankruptcy.results==test.bankruptcy$class)/nrow(test.bankruptcy)
require(gbm)
?gbm
?xgboost
?gbm
gbmform <- gbm(class~., data = anneal.train, n.trees = 100, interaction.depth = 1)
anneal.train
gbmform <- gbm(class~., data = anneal.x, n.trees = 100, interaction.depth = 1)
anneal.x
gbmform <- gbm(class~., data = as.matrix(anneal.x), n.trees = 100, interaction.depth = 1)
gbmform <- gbm(anneal.train_class~., data = as.matrix(anneal.x), n.trees = 100, interaction.depth = 1)
anneal.x
gbmform <- gbm(anneal.train_class~., data = anneal.x1, n.trees = 100, interaction.depth = 1)
gbmform <- gbm(anneal.train_class~., data = anneal.x, n.trees = 100, interaction.depth = 1)
anneal.x
anneal.x1
anneal.x1 <- anneal.a[1:648,]
anneal.y1 <- anneal.a[649:898,]
gbmform <- gbm(anneal.train_class~., data = anneal.x1, n.trees = 100, interaction.depth = 1)
traceback
traceback()
gbm
gbmform <- gbm(parkinsons.trainlabel~., data = train.parkinsons, n.trees = 100, interaction.depth = 1)
parkinsons.testlabel <- as.factor(test.parkinsons$total_UPDRS)
parkinsons.trainlabel <- as.factor(train.parkinsons$total_UPDRS)
gbmform <- gbm(parkinsons.trainlabel~., data = train.parkinsons, n.trees = 100, interaction.depth = 1)
gbmform <- gbm(parkinsons.trainlabel~., data = train.parkinsons, n.trees = 100, interaction.depth = 1,n.minobsinnode = 10, shrinkage =0.1, distribution = "gaussian")
gbmpredict <- predict(gbmform, test.parkinsons[,-c("total_UPDRS")])
gbmpredict <- predict(gbmform, test.parkinsons[,-c("total_UPDRS")], n.trees = 100)
test.parkinsons
gbmform <- gbm(total_UPDRS~., data = train.parkinsons, n.trees = 100, interaction.depth = 1,n.minobsinnode = 10, shrinkage =0.1, distribution = "gaussian")
gbmpredict <- predict(gbmform, test.parkinsons[,-c("total_UPDRS")], n.trees = 100)
gbmpredict
sum((gbmpredict-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
parkinsons.gbm.model <- gbm(total_UPDRS~., data = train.parkinsons, n.trees = 100, interaction.depth = 1,n.minobsinnode = 10, shrinkage =0.1, distribution = "gaussian")
parkinsons.gbm.pred <- predict(parkinsons.gbm.model, test.parkinsons[,-c("total_UPDRS")], n.trees = 100)
sum((parkinsons.gbm.pred-test.parkinsons$total_UPDRS)^2)/nrow(test.parkinsons)
anneal.gbm.model <- gbm(class~., data = train.anneal, n.trees = 100, interaction.depth = 1,n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
anneal.gbm.model <- gbm(class~., data = anneal.train, n.trees = 100, interaction.depth = 1,n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
anneal.gbm.model <- gbm(anneal.train$class~., data = anneal.x, n.trees = 100, interaction.depth = 1,n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
anneal.gbm.pred <- predict(anneal.gbm.model, anneal.y, n.trees = 100)
anneal.gbm.pred <- predict(anneal.gbm.model, anneal.y, n.trees = 100, type = "class")
anneal.gbm.pred <- predict(anneal.gbm.model, anneal.y, n.trees = 100, type = "response")
anneal.gbm.pred
anneal.gbm.pred <- predict(anneal.gbm.model, anneal.y, n.trees = 100, type = "link")
anneal.gbm.pred
anneal.gbm.model <- gbm(anneal.train$class~., data = anneal.x, n.trees = 100, interaction.depth = 1,n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
anneal.gbm.pred <- predict(anneal.gbm.model, anneal.y, n.trees = 100, type = "response")
table(anneal.gbm.pred, anneal.test_class)
sum(anneal.gbm.pred==anneal.test_class)/nrow(anneal.penalized.y1)
anneal.gbm.model <- gbm(anneal.train$class~., data = anneal.x, n.trees = 100, interaction.depth = 1,n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
anneal.gbm.model
anneal.gbm.pred <- predict(anneal.gbm.model, anneal.y, n.trees = 100, type = "response")
anneal.gbm.pred
anneal.gbm.model <- gbm(anneal.train$class~., data = anneal.x, n.trees = 100, type = "response", interaction.depth = 1,n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
anneal.gbm.model <- gbm(anneal.train$class~., data = anneal.x, n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
anneal.gbm.pred <- predict(anneal.gbm.model, anneal.y, n.trees = 100, type = "response")
table(anneal.gbm.pred, anneal.test_class)
sum(anneal.gbm.pred==anneal.test_class)/nrow(anneal.penalized.y1)
anneal.gbm.prob <- predict(anneal.gbm.model, anneal.y, n.trees = 100, type = "response")
anneal.gbm.pred <- apply(anneal.gbm.prob, 1, which.max)
anneal.gbm.pred
table(anneal.gbm.pred, anneal.test_class)
sum(anneal.gbm.pred==anneal.test_class)/nrow(anneal.penalized.y1)
digit.train
#### boost
digit.gbm.model <- gbm(class~., data = train.digit, n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
names.digit <- colnames(train.digit)
names.digit
digit.names <- as.data.table(cbind(names.digit, ifelse(n <- sapply(digit.train, function(x) length(levels(x))) == 1, "DROP", "NODROP")))
digit.names <- as.data.table(cbind(names.digit, ifelse(n <- sapply(train.digit, function(x) length(levels(x))) == 1, "DROP", "NODROP")))
digit.names
train.digit
#### boost
digit.gbm.model <- gbm(class~., data = , n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
#### boost
digit.gbm.model <- gbm(class~., data = train.digit, n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
#### boost
digit.gbm.model <- gbm(class~., data = train.digit[,c(-1,-40)], n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1, distribution = "multinomial")
digit.gbm.prob <- predict(digit.gbm.model, test.digit[,-c("class")], n.trees = 100, type = "response")
test.digit
digit.gbm.prob <- predict(digit.gbm.model, test.digit[,-65], n.trees = 100, type = "response")
digit.gbm.pred <- apply(digit.gbm.prob, 1, which.max)
table(digit.gbm.pred, digit.test_class)
table(digit.gbm.pred, test.digit$class)
table(digit.gbm.pred, test.digit$class)
sum(digit.gbm.pred==test.digit$class)/nrow(test.digit)
test.digit$class
table(digit.gbm.pred-1, test.digit$class)
sum((digit.gbm.pred-1)==test.digit$class)/nrow(test.digit)
train.bankruptcy$class
bankruptcy.penalized_model <- glmnet(x = as.matrix(train.bankruptcy[,-c("class")]), y = train.bankruptcy$class, family = "bernoulli")
bankruptcy.penalized_model <- glmnet(x = as.matrix(train.bankruptcy[,-c("class")]), y = train.bankruptcy$class, family = "binomial")
bankruptcy.results <- predict(bankruptcy.penalized_model,newx =  as.matrix(test.bankruptcy[,-c("class")]), s = 0.01, type = "class")
table(bankruptcy.results, test.bankruptcy$class)
sum(bankruptcy.results==test.bankruptcy$class)/nrow(test.bankruptcy)
#### boost
bankruptcy.gbm.model <- gbm(class~., data = train.bankruptcy[,c(-1,-40)], n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1, distribution = "bernoulli")
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-65], n.trees = 100, type = "response")
bankruptcy.gbm.prob
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-65], n.trees = 100)
bankruptcy.gbm.prob
train.bankruptcy
#### boost
bankruptcy.gbm.model <- gbm(class~., data = train.bankruptcy, n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1, distribution = "bernoulli")
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-65], n.trees = 100)
bankruptcy.gbm.prob
#### boost
bankruptcy.gbm.model <- gbm(class~., data = train.bankruptcy, n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1)
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-65], n.trees = 100)
bankruptcy.gbm.prob
train.bankruptcy
#### boost
train.bankruptcy$class <- as.character(train.bankruptcy$class)
bankruptcy.gbm.model <- gbm(class~., data = train.bankruptcy, n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1)
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-65], n.trees = 100)
bankruptcy.gbm.prob
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-65], n.trees = 100, type = "response")
bankruptcy.gbm.prob
bankruptcy.gbm.pred <- apply(bankruptcy.gbm.prob, 1, which.max)
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-65], n.trees = 100, type = "response")
bankruptcy.gbm.prob
bankruptcy.gbm.pred <- (bankruptcy.gbm.prob >= 0.5)*1
table(bankruptcy.gbm.pred, test.bankruptcy$class)
sum((bankruptcy.gbm.pred)==test.bankruptcy$class)/nrow(test.bankruptcy)
train.bankruptcy.gbm <- as.character(train.bankruptcy$class)
bankruptcy.gbm.model <- gbm(train.bankruptcy.gbm~., data = train.bankruptcy[,65], n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1)
train.bankruptcy.gbm
bankruptcy.gbm.model <- gbm(class~., data = train.bankruptcy[,], n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1)
###### rf
rf_bankruptcy <- randomForest(train.bankruptcy[,-c("class")], y = train.bankruptcy$class, ntree = 500, proximity = TRUE)
####### treee
tree.bankruptcy <- tree(class~., data=train.bankruptcy)
######### svm
bankruptcy.svmfit <- svm(class~., data = train.bankruptcy, kernel = "linear", cost = 0.1, scale = TRUE)
train.bankruptcy$classchar <- as.character(train.bankruptcy$class)
######### BANKRUPTCY
bankruptcy <- as.data.table(readARFF("Data/Bankruptcy/1year.arff"))
set.seed(15)
bankruptcy <- bankruptcy[complete.cases(bankruptcy)]
bankruptcy.test.index <- sample(nrow(bankruptcy), 750)
train.bankruptcy <- bankruptcy[-bankruptcy.test.index,]
test.bankruptcy <- bankruptcy[bankruptcy.test.index,]
train.bankruptcy
bankruptcy <- as.data.table(readARFF("Data/Bankruptcy/1year.arff"))
set.seed(15)
bankruptcy <- bankruptcy[complete.cases(bankruptcy)]
bankruptcy.test.index <- sample(nrow(bankruptcy), 750)
train.bankruptcy <- bankruptcy[-bankruptcy.test.index,]
test.bankruptcy <- bankruptcy[bankruptcy.test.index,]
train.bankruptcy$classchar <- as.character(train.bankruptcy$class)
train.bankruptcy
test.bankruptcy$classchar <- as.character(test.bankruptcy$class)
test.bankruptcy
bankruptcy.penalized_model <- glmnet(x = as.matrix(train.bankruptcy[,-c("class", "classchar")]), y = train.bankruptcy$class, family = "binomial")
bankruptcy.results <- predict(bankruptcy.penalized_model,newx =  as.matrix(test.bankruptcy[,-c("class", "classchar")]), s = 0.01, type = "class")
table(bankruptcy.results, test.bankruptcy$class)
sum(bankruptcy.results==test.bankruptcy$class)/nrow(test.bankruptcy)
bankruptcy.gbm.model <- gbm(classchar~., data = train.bankruptcy[,-c("class")], n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1)
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-c("class")], n.trees = 100, type = "response")
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-c("class","classchar")], n.trees = 100, type = "response")
bankruptcy.gbm.pred <- (bankruptcy.gbm.prob >= 0.5)*1
table(bankruptcy.gbm.pred, test.bankruptcy$class)
sum((bankruptcy.gbm.pred)==test.bankruptcy$class)/nrow(test.bankruptcy)
bankruptcy.gbm.model <- gbm(class~., data = train.bankruptcy[,-c("class")], n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1)
bankruptcy.gbm.model <- gbm(class~., data = train.bankruptcy, n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1)
bankruptcy.gbm.model <- gbm(class~., data = train.bankruptcy[,-c((classchar))], n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1)
bankruptcy.gbm.model <- gbm(class~., data = train.bankruptcy[,-c("classchar")], n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1)
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-c("class","classchar")], n.trees = 100, type = "response")
bankruptcy.gbm.pred <- (bankruptcy.gbm.prob >= 0.5)*1
bankruptcy.gbm.pred
bankruptcy.gbm.model <- gbm(classchar~., data = train.bankruptcy[,-c("class")], n.trees = 100, interaction.depth = 1,
n.minobsinnode = 10, shrinkage =0.1)
bankruptcy.gbm.prob <- predict(bankruptcy.gbm.model, test.bankruptcy[,-c("class","classchar")], n.trees = 100, type = "response")
bankruptcy.gbm.pred <- (bankruptcy.gbm.prob >= 0.5)*1
table(bankruptcy.gbm.pred, test.bankruptcy$class)
sum((bankruptcy.gbm.pred)==test.bankruptcy$class)/nrow(test.bankruptcy)
rf_bankruptcy <- randomForest(train.bankruptcy[,-c("class","classchar")], y = train.bankruptcy$class, ntree = 500, proximity = TRUE)
bankruptcy.prob_classes <- predict(rf_bankruptcy, test.bankruptcy[,-c("class","classchar")])
table(bankruptcy.prob_classes, test.bankruptcy$class)
sum(bankruptcy.prob_classes==test.bankruptcy$class)/nrow(test.bankruptcy)
######### svm
bankruptcy.svmfit <- svm(class~., data = train.bankruptcy[,-c("classchar")], kernel = "linear", cost = 0.1, scale = TRUE)
bankruptcy.svmpred <- predict(bankruptcy.svmfit, test.bankruptcy[,-c("class","classchar")])
table(bankruptcy.svmpred,test.bankruptcy$class)
sum(bankruptcy.svmpred==test.bankruptcy$class)/nrow(test.bankruptcy)
####### treee
tree.bankruptcy <- tree(class~., data=train.bankruptcy[,-c("classchar")])
bankruptcy.pred <- predict(tree.bankruptcy, test.bankruptcy[,-c("class", "classchar")], type = "class")
table(bankruptcy.pred,test.bankruptcy$class)
sum(bankruptcy.pred==test.bankruptcy$class)/nrow(test.bankruptcy)
?tree
require(rpart)
?rpart
anneal.tree.control <- rpart.control(minsplit = 10, cp = 0.1)
tree.anneal <- rpart(class~., anneal.train, control = anneal.tree.control)
anneal.test
anneal.tree.pred <- predict(tree.anneal, anneal.test[,-c("class")], type = "class")
anneal.tree.pred <- predict(tree.anneal, anneal.test[,-c("class")])
anneal.tree.pred
?predict.rpart
anneal.tree.pred <- predict(tree.anneal, anneal.test[,-c("class")], type = "class")
tree.anneal <- rpart(class~., anneal.train, control = anneal.tree.control, type = "class")
?rpart
tree.anneal <- rpart(class~., anneal.train, control = anneal.tree.control, method = "class")
anneal.tree.pred <- predict(tree.anneal, anneal.test[,-c("class")], type = "class")
table(anneal.tree.pred,anneal.test$class)
sum(anneal.tree.pred==anneal.test$class)/nrow(anneal.test)
anneal.tree.conf.mat <- table(anneal.tree.pred,anneal.test$class)
anneal.tree.error <- sum(anneal.tree.pred==anneal.test$class)/nrow(anneal.test)
anneal.tree.conf.mat
anneal.tree.error
anneal.tree.control
